import streamlit as st
import sys
import os.path
import argparse
import ast
import struct  # æ·»åŠ ç¼ºå¤±çš„ struct å¯¼å…¥
from scipy.special import rel_entr, softmax
import gzip
import pickle
from scipy.stats.mstats import mquantiles_cimj
from scipy.stats import bayes_mvs
from scipy.stats import t as student_t
import random
import time

# å¯¼å…¥ llama_cppï¼Œå¦‚æœå¯¼å…¥å¤±è´¥åˆ™æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
try:
    import llama_cpp
    import llama_cpp._internals as _llama_internals

    if not getattr(_llama_internals.LlamaModel, "_safe_close_patched", False):
        _original_llama_close = _llama_internals.LlamaModel.close

        def _safe_llama_close(self):
            try:
                _original_llama_close(self)
            except AttributeError:
                sampler = getattr(self, "sampler", None)
                if sampler is not None:
                    custom_samplers = list(getattr(self, "custom_samplers", []))
                    for i, _ in reversed(custom_samplers):
                        llama_cpp.llama_sampler_chain_remove(sampler, i)
                    if hasattr(self, "custom_samplers"):
                        self.custom_samplers.clear()
                exit_stack = getattr(self, "_exit_stack", None)
                if exit_stack is not None:
                    exit_stack.close()

        _llama_internals.LlamaModel.close = _safe_llama_close
        _llama_internals.LlamaModel._safe_close_patched = True
except ImportError:
    st.error("è¯·ç¡®ä¿å·²å®‰è£… llama-cpp-python: pip install llama-cpp-python")
    sys.exit(1)

import numpy as np

def kl_div(p, q):
    """è®¡ç®—å•ä¸ª token ä¸Šä¸¤ä¸ªæ¨¡å‹ logits çš„ KL æ•£åº¦ã€‚"""
    p = softmax(p)
    q = softmax(q)
    return np.sum(rel_entr(p, q))

def write_header(f, args, ctx, vocab_len, batch):
    """æŠŠè¿è¡Œé…ç½®å†™å…¥åˆ°è¾“å‡ºæ–‡ä»¶å¤´éƒ¨ï¼Œæ–¹ä¾¿ä¹‹åå¤ç°è®¡ç®—ç¯å¢ƒã€‚"""
    f.write("llama_kl_divergence_v1\n".encode('utf-8'))
    d = vars(args)
    d["n_ctx"] = ctx
    d["n_vocab"] = vocab_len
    d["n_batch"] = batch
    f.write((str(d)+"\n").encode('utf-8'))

def read_header(f):
    """ä»å‹ç¼©æ–‡ä»¶å¼€å¤´è¯»å–å¹¶æ¢å¤è¿è¡Œé…ç½®ã€‚"""
    header = "llama_kl_divergence_v1\n".encode('utf-8')
    if f.read(len(header)) != header:
        raise ValueError("Invalid header in input logit file")
    args = ast.literal_eval(f.readline().decode('utf-8').strip())
    return args

def write_logits(f, tokens, logits):
    """æŠŠ token åºåˆ—åŠå¯¹åº” logits åºåˆ—ä»¥äºŒè¿›åˆ¶å½¢å¼å†™å…¥æ–‡ä»¶ã€‚"""
    f.write(struct.pack("<I", len(tokens)))
    f.write(struct.pack("<I", len(logits)))
    f.write(struct.pack("<I", len(logits[0])))
    t = np.array(tokens, dtype=np.uint32).tobytes()
    assert len(t) == 4 * len(tokens)
    f.write(t)
    l = np.array(logits, dtype=np.float32).tobytes()
    assert len(l) == 4 * len(logits) * len(logits[0])
    f.write(l)

def read_logits(f):
    """ä»æ–‡ä»¶ä¸­è¯»å–ä¸€æ®µ token åºåˆ—åŠå…¶ logitsã€‚åˆ°è¾¾æ–‡ä»¶å°¾æ—¶è¿”å› (None, None)ã€‚"""
    n_tokens = f.read(4)
    if len(n_tokens) != 4:
        # EOF
        return None, None
    n_tokens = struct.unpack("<I", n_tokens)[0]
    n_logits = struct.unpack("<I",f.read(4))[0]
    n_vocab = struct.unpack("<I",f.read(4))[0]
    tokens = [int(i) for i in np.frombuffer(f.read(n_tokens * 4), dtype=np.uint32)]
    logits = np.frombuffer(f.read(n_logits * n_vocab * 4), dtype=np.float32).reshape(n_logits, n_vocab)
    return tokens, logits

class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨ï¼Œç”¨äºæ­£ç¡®å¤„ç†æ¨¡å‹çš„åˆ›å»ºå’Œé”€æ¯"""
    def __init__(self, model_path, ctx, batch_size, gpu_layers=0):
        self.model = None
        self.model_path = model_path
        self.ctx = ctx
        self.batch_size = batch_size
        self.gpu_layers = gpu_layers
        
    def __enter__(self):
        status_text = st.empty()
        status_text.text("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        
        # ç¦ç”¨ verbose ä»¥é¿å…è¿‡å¤šçš„è¾“å‡ºä¿¡æ¯
        self.model = llama_cpp.Llama(
            model_path=self.model_path,
            n_ctx=self.ctx,
            n_batch=self.batch_size,
            logits_all=True,
            n_gpu_layers=self.gpu_layers,
            verbose=False
        )
        
        status_text.text("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        return self.model
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.model is not None:
            # æ˜¾å¼å…³é—­æ¨¡å‹ä»¥é¿å…èµ„æºæ³„æ¼
            try:
                self.model.close()
            except AttributeError:
                pass
            finally:
                self.model = None

def run_kl_analysis(args):
    """ä¸»æµç¨‹ï¼šå‡†å¤‡æ¨¡å‹ã€è¯»å–/ç”Ÿæˆè¾“å…¥ã€è®¡ç®— KL æ•£åº¦å¹¶è¾“å‡ºç»Ÿè®¡ç»“æœã€‚"""
    ctx = args.n_ctx
    progress_bar = st.progress(0)
    status_text = st.empty()

    read_file = None
    if args.read_path is not None:
        status_text.text(f"æ­£åœ¨åŠ è½½å‚è€ƒ logits æ–‡ä»¶: {args.read_path}")
        read_file = gzip.open(args.read_path, "rb")
        input_args = read_header(read_file)
        ctx = input_args["n_ctx"]

    # ä½¿ç”¨æ¨¡å‹ç®¡ç†å™¨æ¥å¤„ç†æ¨¡å‹çš„åˆ›å»ºå’Œé”€æ¯
    with ModelManager(args.model, ctx, args.n_batch, args.n_gpu_layers) as model:
        if model is None:
            st.error("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
            return
            
        model_name = os.path.split(args.model)[1]

        tokens = None
        if args.text_path and args.read_path is None:
            status_text.text(f"æ­£åœ¨è¯»å–æ–‡æœ¬æ–‡ä»¶: {args.text_path}")
            with open(args.text_path, "r") as f:
                prompt = f.read()
            tokens = model.tokenize(prompt.encode('utf-8'))
            bos = model.token_bos()
            b = 1 if bos is not None else 0
            tokens = [tokens[i:i+ctx-b] for i in range(0, len(tokens), ctx-b)]
            random.seed(123)
            if bos is not None:
                for i in range(len(tokens)):
                    tokens[i].insert(0, bos)
            random.shuffle(tokens)

        write_file = None
        if args.write_path is not None:
            write_file = gzip.open(args.write_path, "wb")
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å…ˆè·å–æ¨¡å‹ä¿¡æ¯ï¼Œæ‰€ä»¥éœ€è¦åœ¨ with è¯­å¥å†…éƒ¨è°ƒç”¨
            vocab_len = model.n_vocab()
            batch_size = args.n_batch
            write_header(write_file, args, model.n_ctx(), vocab_len, batch_size)

        def next_sample():
            """æŒ‰éœ€ç”Ÿæˆä¸‹ä¸€æ®µè¦è¯„ä¼°çš„ tokens åŠå‚è€ƒ logitsã€‚"""
            if read_file is not None:
                while True:
                    try:
                        t, logits = read_logits(read_file)
                    except EOFError:
                        print("EOF at unexpected location")
                        return
                    if t is None:
                        return
                    yield logits, t
            elif tokens is not None:
                for i, t in enumerate(tokens):
                    progress_bar.progress((i + 1) / len(tokens))
                    status_text.text(f"å¤„ç†æ–‡æœ¬ç‰‡æ®µ {i+1}/{len(tokens)}")
                    yield None, t

        alpha = 0.01
        kls = []
        top1 = 0
        top5 = 0
        top10 = 0
        eval_top5 = 0
        eval_top10 = 0
        samples = 0
        written = 0
        written_tokens = 0
        i = 0
        errors = 0

        max_tokens = args.n_tokens if args.n_tokens >= 0 else float('inf')
        
        status_text.text("å¼€å§‹è®¡ç®— KL æ•£åº¦...")
        try:
            for logits, chunk in next_sample():
                model.reset()
                output = model.eval(chunk)
                eval_logits = model.eval_logits
                
                if np.any(np.isnan(eval_logits)):
                    errors += 1
                    print("Nan in logits!")
                    eval_logits = np.nan_to_num(eval_logits)
                    
                if write_file:
                    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ç¡®ä¿æ¨¡å‹æœ‰ eval_tokens å±æ€§
                    if hasattr(model, 'eval_tokens'):
                        write_logits(write_file, model.eval_tokens, eval_logits)
                        written_tokens += len(model.eval_tokens)
                        written += 1
                    
                if logits is not None:
                    new_kls = [kl_div(eval_logits[i], logits[i]) for i in range(len(logits))]
                    if np.any(np.isnan(new_kls)):
                        errors += 1
                        print("Nan in computed kls!")
                        new_kls = np.nan_to_num(new_kls)
                    kls.extend(new_kls)
                    samples += len(logits)
                    
                    eval_argmax = np.argmax(eval_logits, axis=-1)
                    ref_argmax = np.argmax(logits, axis=-1)
                    eval_part5 = np.argpartition(eval_logits, -5, axis=-1)[:,-5:]
                    ref_part5 = np.argpartition(logits, -5, axis=-1)[:,-5:]
                    eval_part10 = np.argpartition(eval_logits, -10, axis=-1)[:,-10:]
                    ref_part10 = np.argpartition(logits, -10, axis=-1)[:,-10:]
                    
                    top1 += sum([eval_argmax[i] == ref_argmax[i] for i in range(len(logits))])
                    top5 += sum([ref_argmax[i] in eval_part5[i] for i in range(len(logits))])
                    top10 += sum([ref_argmax[i] in eval_part10[i] for i in range(len(logits))])
                    eval_top5 += sum([eval_argmax[i] in ref_part5[i] for i in range(len(logits))])
                    eval_top10 += sum([eval_argmax[i] in ref_part10[i] for i in range(len(logits))])
                    
                if samples >= max_tokens:
                    break
                    
        except KeyboardInterrupt:
            status_text.text("è®¡ç®—è¢«ç”¨æˆ·ä¸­æ–­")
        
        if write_file:
            write_file.close()
        if read_file:
            read_file.close()

        def bin_conf(p, n, z):
            if p == 0:
                p = 1 / (n + 2)
            if p == 1:
                p = 1 - 1 / (n + 2)
            return z * np.sqrt(p*(1-p)/n)

        if len(kls) > 0:
            z = student_t.ppf(1 - alpha/2, samples)
            
            st.subheader("ğŸ“Š è®¡ç®—ç»“æœ")
            
            with st.expander("æ¨¡å‹ä¿¡æ¯"):
                st.write(f"**æ¨¡å‹åç§°**: {model_name}")
                
                # å®‰å…¨è·å–æ¨¡å‹å¤§å°ä¿¡æ¯
                try:
                    model_size = llama_cpp.llama_model_size(model.model)
                    params = llama_cpp.llama_n_params(model.model)
                    bpw = 8 * model_size / params if params > 0 else 0
                    st.write(f"**æ¨¡å‹å¤§å°**: {model_size / 1024**3:.3g} GiB (BPW {bpw:.2f})")
                except:
                    st.write("**æ¨¡å‹å¤§å°**: æ— æ³•è·å–")
                
                st.write(f"**Token æ•°é‡**: {samples}")
            
            with st.expander("ğŸ”¥ KL æ•£åº¦ç»Ÿè®¡"):
                m, _, __ = bayes_mvs(kls, 1-alpha)
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("å‡å€¼", f"{m[0]:.6g}")
                    z_interval = z * np.sqrt(np.mean([k**2 for k in kls])/len(kls))
                    st.caption(f"ç½®ä¿¡åŒºé—´: [{m[1][0]:.6g} - {m[1][1]:.6g}]")
                
                with col2:
                    q90 = np.quantile(kls, 0.90)
                    q95 = np.quantile(kls, 0.95)
                    st.metric("Q90", f"{q90:.4g}")
                    st.metric("Q95", f"{q95:.4g}")
                
                with col3:
                    q99 = np.quantile(kls, 0.99)
                    max_kl = np.max(kls)
                    st.metric("Q99", f"{q99:.4g}")
                    st.metric("æœ€å¤§å€¼", f"{max_kl:.4g}")

            with st.expander("ğŸ¯ å‡†ç¡®ç‡ç»Ÿè®¡"):
                z_val = student_t.ppf(1 - alpha/2, samples)
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write("**å‚è€ƒæ¨¡å‹ Top Token åœ¨è¯„ä¼°æ¨¡å‹ä¸­çš„ä½ç½®**")
                    st.metric(f"Top 1 å‡†ç¡®ç‡", f"{top1 / samples:.4g}")
                    st.caption(f"ç½®ä¿¡åŒºé—´: Â±{bin_conf(top1/samples, samples, z_val):.4g}")
                    
                    st.metric(f"Top 5 å‡†ç¡®ç‡", f"{top5 / samples:.4g}")
                    st.caption(f"ç½®ä¿¡åŒºé—´: Â±{bin_conf(top5/samples, samples, z_val):.4g}")
                    
                    st.metric(f"Top 10 å‡†ç¡®ç‡", f"{top10 / samples:.4g}")
                    st.caption(f"ç½®ä¿¡åŒºé—´: Â±{bin_conf(top10/samples, samples, z_val):.4g}")
                
                with col2:
                    st.write("**è¯„ä¼°æ¨¡å‹ Top Token åœ¨å‚è€ƒæ¨¡å‹ä¸­çš„ä½ç½®**")
                    st.metric("Top 5 è¦†ç›–ç‡", f"{eval_top5 / samples:.4g}")
                    st.caption(f"ç½®ä¿¡åŒºé—´: Â±{bin_conf(eval_top5/samples, samples, z_val):.4g}")
                    
                    st.metric("Top 10 è¦†ç›–ç‡", f"{eval_top10 / samples:.4g}")
                    st.caption(f"ç½®ä¿¡åŒºé—´: Â±{bin_conf(eval_top10/samples, samples, z_val):.4g}")
                
                if errors > 0:
                    st.warning(f"âš ï¸ æ£€æµ‹åˆ° {errors} ä¸ªé”™è¯¯ï¼ˆNaN å€¼ï¼‰")

            with st.expander("ğŸ’¾ æ•°æ®å¯¼å‡º"):
                if st.button("ä¿å­˜ KL æ•£åº¦æ•°æ®"):
                    output_file = f"{model_name}.kls.p"
                    with open(output_file, 'wb') as f:
                        pickle.dump(kls, f)
                    st.success(f"KL æ•£åº¦æ•°æ®å·²ä¿å­˜åˆ° {output_file}")
        else:
            st.error("æ²¡æœ‰å¯ç”¨çš„ KL æ•£åº¦è®¡ç®—ç»“æœ")

# Streamlit ç•Œé¢è®¾ç½®
st.set_page_config(
    page_title="LLM KL æ•£åº¦åˆ†æå·¥å…·",
    page_icon="ğŸ§®",
    layout="wide"
)

st.title("ğŸ§® LLM KL æ•£åº¦åˆ†æå·¥å…·")
st.markdown("""
è¿™æ˜¯ä¸€ä¸ªç”¨äºæ¯”è¾ƒä¸¤ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹è¾“å‡ºçš„å¯¹æ•°å‡ ç‡ï¼ˆlogitsï¼‰ä¹‹é—´å·®å¼‚çš„å·¥å…·ã€‚
KL æ•£åº¦å¯ä»¥è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼Œå¸®åŠ©æˆ‘ä»¬é‡åŒ–ä¸åŒç‰ˆæœ¬æˆ–é…ç½®çš„æ¨¡å‹ä¹‹é—´çš„å˜åŒ–ç¨‹åº¦ã€‚

## ä½¿ç”¨è¯´æ˜
1. **å†™å…¥æ¨¡å¼**: é¦–å…ˆä½¿ç”¨é«˜ç²¾åº¦æ¨¡å‹è¿è¡Œå¹¶ä¿å­˜ logits åˆ°æ–‡ä»¶
2. **è¯»å–æ¨¡å¼**: ç„¶åç”¨é‡åŒ–æ¨¡å‹è¿è¡Œï¼Œè¯»å–ä¹‹å‰ä¿å­˜çš„æ–‡ä»¶æ¥è®¡ç®— KL æ•£åº¦

âš ï¸ **æ³¨æ„**: å¦‚æœé‡åˆ°æ¨¡å‹åŠ è½½é”™è¯¯ï¼Œè¯·æ£€æŸ¥ llama_cpp ç‰ˆæœ¬å…¼å®¹æ€§ã€‚
""")

# ä¾§è¾¹æ å‚æ•°è®¾ç½®
st.sidebar.header("âš™ï¸ å‚æ•°è®¾ç½®")

mode = st.sidebar.selectbox(
    "è¿è¡Œæ¨¡å¼",
    ["å†™å…¥æ¨¡å¼", "è¯»å–æ¨¡å¼"],
    help="é€‰æ‹©è¦è¿è¡Œçš„æ¨¡å¼ï¼šå†™å…¥æ¨¡å¼ç”Ÿæˆ logits æ–‡ä»¶ï¼Œè¯»å–æ¨¡å¼è®¡ç®— KL æ•£åº¦"
)

args = argparse.Namespace()

# åŸºç¡€å‚æ•°
st.sidebar.subheader("åŸºç¡€è®¾ç½®")
args.model = st.sidebar.text_input(
    "æ¨¡å‹è·¯å¾„",
    placeholder="/path/to/model.gguf",
    help="æŒ‡å®š LLM æ¨¡å‹çš„æ–‡ä»¶è·¯å¾„ (GGUF æ ¼å¼)"
)
args.n_ctx = st.sidebar.number_input(
    "ä¸Šä¸‹æ–‡å¤§å°",
    min_value=1,
    max_value=8192,
    value=512,
    help="æ¨¡å‹çš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦"
)
args.n_batch = st.sidebar.number_input(
    "æ‰¹å¤„ç†å¤§å°", 
    min_value=1,
    max_value=2048,
    value=512,
    help="æ¨ç†æ—¶çš„æ‰¹å¤„ç†å¤§å°"
)

# GPU å±‚æ•°
st.sidebar.subheader("GPU åŠ é€Ÿ")
args.n_gpu_layers = st.sidebar.slider(
    "GPU å±‚æ•°",
    min_value=0,
    max_value=100,
    value=0,
    help="å¸è½½åˆ° GPU çš„å±‚æ•°æ•°é‡ï¼ˆ0 è¡¨ç¤ºå…¨éƒ¨ä½¿ç”¨ CPUï¼‰"
)

# Token æ•°é‡é™åˆ¶
st.sidebar.subheader("è¯„ä¼°è®¾ç½®")
args.n_tokens = st.sidebar.number_input(
    "Token é™åˆ¶",
    min_value=-1,
    max_value=100000,
    value=-1,
    help="è¦è¯„ä¼°çš„ Token æ•°é‡ï¼ˆ-1 è¡¨ç¤ºå…¨éƒ¨å¤„ç†ï¼‰"
)

# é¢„è®¾å¯é€‰å‚æ•°ï¼Œç¡®ä¿å±æ€§åœ¨ä¸¤ç§æ¨¡å¼ä¸‹éƒ½å­˜åœ¨
args.text_path = None
args.read_path = None
args.write_path = None

# æ ¹æ®æ¨¡å¼æ˜¾ç¤ºä¸åŒçš„å‚æ•°
if mode == "å†™å…¥æ¨¡å¼":
    st.sidebar.subheader("ğŸ“ å†™å…¥è®¾ç½®")
    args.text_path = st.sidebar.text_input(
        "æ–‡æœ¬æ•°æ®é›†è·¯å¾„",
        placeholder="/path/to/dataset.txt",
        help="åŒ…å«è¾“å…¥æ–‡æœ¬çš„æ–‡ä»¶è·¯å¾„"
    )
    args.write_path = st.sidebar.text_input(
        "è¾“å‡º logits æ–‡ä»¶è·¯å¾„",
        placeholder="logits.gz",
        help="ä¿å­˜æ¨¡å‹è¾“å‡ºçš„å‹ç¼© logits æ–‡ä»¶è·¯å¾„"
    )
    args.read_path = None
else:
    st.sidebar.subheader("ğŸ“– è¯»å–è®¾ç½®")
    args.read_path = st.sidebar.text_input(
        "å‚è€ƒ logits æ–‡ä»¶è·¯å¾„",
        placeholder="/path/to/logits.gz",
        help="ä¹‹å‰ç”Ÿæˆçš„å‚è€ƒæ¨¡å‹ logits æ–‡ä»¶è·¯å¾„"
    )
    args.write_path = None

args.verbose = False

# å‚æ•°éªŒè¯
if st.sidebar.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary"):
    if not args.model:
        st.error("âŒ è¯·æŒ‡å®šæ¨¡å‹è·¯å¾„")
    elif mode == "å†™å…¥æ¨¡å¼" and (not args.text_path or not args.write_path):
        st.error("âŒ å†™å…¥æ¨¡å¼ä¸‹éœ€è¦æŒ‡å®šæ–‡æœ¬æ•°æ®é›†å’Œè¾“å‡ºæ–‡ä»¶è·¯å¾„")
    elif mode == "è¯»å–æ¨¡å¼" and not args.read_path:
        st.error("âŒ è¯»å–æ¨¡å¼ä¸‹éœ€è¦æŒ‡å®šå‚è€ƒ logits æ–‡ä»¶è·¯å¾„")
    else:
        try:
            # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if args.text_path and not os.path.exists(args.text_path):
                raise FileNotFoundError(f"æ–‡æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {args.text_path}")
            if args.read_path and not os.path.exists(args.read_path):
                raise FileNotFoundError(f"å‚è€ƒ logits æ–‡ä»¶ä¸å­˜åœ¨: {args.read_path}")
            
            # æ£€æŸ¥è¾“å‡ºè·¯å¾„æ˜¯å¦å·²å­˜åœ¨
            if args.write_path and os.path.exists(args.write_path):
                raise FileExistsError(f"è¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ï¼Œè¯·åˆ é™¤æˆ–æ›´æ¢è·¯å¾„: {args.write_path}")
                
            st.info("æ­£åœ¨å‡†å¤‡åˆ†æç¯å¢ƒ...")
            run_kl_analysis(args)
            
        except Exception as e:
            st.error(f"âŒ è¿è¡Œæ—¶é”™è¯¯: {str(e)}")
